{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebb2362",
   "metadata": {},
   "source": [
    "## Commenting and documentation of all 4 successful dags\n",
    "This notebook will introduce the four successful dags that are dumped already and will cover the explanation for the code of these 4 successful dags. There are two dags for dumping tables of POS database, and other two dags for merging iceberg tables.\n",
    "\n",
    "The two dags for POS database are:\n",
    "1. pos_multiple_dump \n",
    "2. pos_large_INSTALLMENT_SCHE_dump\n",
    "\n",
    "The two dags for merging iceberg tables are:\n",
    "\n",
    "1. ALL_POS_S3_to_Iceberg_dag\n",
    "2. pos_large_installment_sche_ICEBERG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e644315",
   "metadata": {},
   "source": [
    "## Brief Introduction of two dags for POS database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea842e9d",
   "metadata": {},
   "source": [
    "pos_multiple_dump: This dag was used for dumping multiple small tables in a parallel manner in S3 storage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca2a71",
   "metadata": {},
   "source": [
    "pos_large_INSTALLMENT_SCHE_dump: This dag was used for dumping one large table in S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320837d4",
   "metadata": {},
   "source": [
    "## Brief Introduction of two dags for iceberge merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4b5071",
   "metadata": {},
   "source": [
    "ALL_POS_S3_to_Iceberg_dag: This dag was used for merging multiple pos small tables into iceberg format in S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5984def1",
   "metadata": {},
   "source": [
    "pos_large_installment_sche_ICEBERG: This dag was used for merging one large table into iceberg format in S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a2042",
   "metadata": {},
   "source": [
    "## Necessary packages for the code of four successful dags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a51506",
   "metadata": {},
   "source": [
    "We used datetime, json, os, sys, re, DAG,Variable,S3Hook, Bashoperator,Pythonoperator,Oracleoperator,OracleHook,k8s,Trigger_Rule,pandas,days_ago and open packages for dumping large table and multiple small tables and merging multiple small tables and a large table into icerberg format in S3 storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a2514",
   "metadata": {},
   "source": [
    "## Explanation for the code of pos_multiple_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d624efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Code:\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'pos'\n",
    "}\n",
    "\n",
    "Since the database is pos, that's why, the owner is pos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e7d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "\n",
    "dag = DAG(\n",
    "        dag_id='pos_multiple_dump',\n",
    "        start_date=days_ago(0),\n",
    "        schedule_interval=None,\n",
    "        catchup=False,\n",
    "      )\n",
    "\n",
    "\n",
    "This code was used for declaring the dag id in order to dump multiple small tables of pos database into s3 storage. The dag_id aka dag name is pos_multiple_dump. The start_date marks the start of the DAG's first data interval, not when tasks in the dag will start running. The time is set with respect to the default timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8942b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "\n",
    "def get_column_data_type(original_schema_dict, column_name):\n",
    "    return original_schema_dict[column_name][\"DATA_TYPE\"]\n",
    "    \n",
    "    \n",
    "This function is used to get the data types for all the columns in original schema dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c47add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code:\n",
    "\n",
    "def get_column_data_scale(original_schema_dict, column_name):\n",
    "    return original_schema_dict[column_name][\"DATA_SCALE\"]\n",
    "    \n",
    "After getting the data type, we defined this function to scale the data column in original schema dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a9a9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters_from_all_column_names(df):\n",
    "    print(\"\")// This prints nothing.\n",
    "    print(\"Removing Special Characters From All Column Names ....\")// This prints the title removing special characters from all column names\n",
    "    for col in df.columns:\n",
    "        print(f\"{col} --> {df[col].dtype}\")// This is used to print the data type of all the columns in the dataframe\n",
    "        df = df.rename(columns={col: re.sub(\"[!@#$&*^%~<>?+=]\", \"\", col)})\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "Overall, this function was used to remove special characters from all column names in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd6a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_original_schema_dict(conn, schema_name, table_name):\n",
    "    # First dictionary named original_schema_dict is defined.\n",
    "    original_schema_dict = {}\n",
    "    try:\n",
    "        # We ran the following sql query to see the schema for the table where we can see the data type, data_scale,data_precision and columnname for         the column names in table.\n",
    "        schema_sql = f\"SELECT COLUMN_NAME, DATA_TYPE, DATA_SCALE, DATA_PRECISION, COLUMN_ID FROM sys.all_tab_columns where OWNER = '{schema_name.upper()}' AND TABLE_NAME = '{table_name.upper()}' ORDER BY COLUMN_ID\"\n",
    "        print(schema_sql)\n",
    "        # Then we connect with oracle and then read a dataframe.\n",
    "        ndf = pd.read_sql(schema_sql, conn)\n",
    "        # Then we ran for loop \n",
    "        for idx, row in ndf.iterrows():\n",
    "            print(row['COLUMN_NAME'], row['DATA_TYPE'], \"DL:\", row['DATA_SCALE'], \" , DP:\", row['DATA_PRECISION'])\n",
    "            val = {\n",
    "                \"DATA_TYPE\": row[\"DATA_TYPE\"],\n",
    "                \"DATA_PRECISION\": row[\"DATA_PRECISION\"],\n",
    "                \"DATA_SCALE\": row[\"DATA_SCALE\"]\n",
    "            }\n",
    "            original_schema_dict[row[\"COLUMN_NAME\"]] = val\n",
    "    except Exception as e:\n",
    "        print(\"[ERROR] while fetching table meta data....\")\n",
    "        print(str(e))\n",
    "    \n",
    "    return original_schema_dict\n",
    "    \n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570070af",
   "metadata": {},
   "source": [
    "In this function, we defined dictionary named original_schema_dict. Also, we ran sql query to see the schema for the table where we saw the data type, data_scale,data_precision and columnname for the column in table. Then we connected with oracle and then read a dataframe. After that, we ran for loop to see the rows of the data, and then saved data type, data precision and data_scale in a seperate dictionary named val. Later, we declared val equal to column name in original schema dictionary to view the calculated original schema for the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019f0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types_to_original(original_schema_dict, df):\n",
    "    print(\"\")\n",
    "    print(\"Converting Data Types ....\")\n",
    "    try:\n",
    "        for col in df.columns:\n",
    "            data_type = get_column_data_type(original_schema_dict, col)\n",
    "            if data_type == \"CHAR\":\n",
    "                data_type = \"str\"\n",
    "            elif data_type == \"ROWID\":\n",
    "                data_type = \"str\"\n",
    "            elif data_type == \"VARCHAR\":\n",
    "                data_type = \"str\"\n",
    "            elif data_type == \"VARCHAR2\":\n",
    "                data_type = \"str\"\n",
    "            elif data_type == \"DATE\":\n",
    "                data_type = \"datetime64[ns]\"\n",
    "            elif data_type == \"LONG\":\n",
    "                df[col] = df[col].fillna(0)\n",
    "                data_type = \"int64\"\n",
    "            elif data_type == \"FLOAT\":\n",
    "                df[col] = df[col].fillna(0)\n",
    "                data_type = \"float64\"\n",
    "            elif data_type == \"NUMBER\":\n",
    "                df[col] = df[col].fillna(0)\n",
    "                df[col] = pd.to_numeric(df[col])\n",
    "                data_type = \"float64\"\n",
    "            else:\n",
    "                data_type = df[col].dtype\n",
    "            \n",
    "            if data_type == \"object\":\n",
    "                data_type = \"str\"\n",
    "\n",
    "            if data_type != df[col].dtype:\n",
    "                print(f\"{col} ({df[col].dtype}) --> {data_type}\")\n",
    "                if data_type == \"datetime64[ns]\":\n",
    "                    df[col] = pd.to_datetime(df[col])\n",
    "                else:\n",
    "                    df = df.astype({col: data_type})\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Conversion Completed\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        print(\"[ERROR] Error occurred while schema converstion.\")\n",
    "        print(f\"[ERROR] {col} ({df[col].dtype}) --> {data_type}\")\n",
    "        raise Exception(str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086d5c36",
   "metadata": {},
   "source": [
    "This function was used to convert data types into original schema. First, we called a function named get_column_data_type to fetch the data type of the columns in dataframe which I discussed earlier. Subsequently, we declared different conditions to change the data type to original type after retrieving the data type from the function named get_column_data_type i.e whenever we got character type data,rowid, varchar and varchar2, we modified the data type into string. Moreover, we converted date type into date time format and filled with null values whenever we got long type, float type and number type values. We kept some of the data type as same as before which did not match any of our given conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a1180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OracleToParquetToS3(schema_name, table_name, target_bucket, file_key):\n",
    "\n",
    "\n",
    "        SQL= f\"SELECT * FROM {schema_name}.{table_name}\"\n",
    "        oracle_conn = OracleHook(oracle_conn_id='con-ora-pos').get_conn()\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Executing SELECT Query...\")\n",
    "        df = pd.read_sql(SQL, oracle_conn)\n",
    "\n",
    "        # Calculate Original Schema Dictionary\n",
    "        print(\"\")\n",
    "        print(\"Fetching Table Meta Data...\")\n",
    "        original_schema_dict = calculate_original_schema_dict(oracle_conn, schema_name, table_name)\n",
    "        if len(original_schema_dict.keys()) == 0:\n",
    "            raise Exception(\"Could not calculate original schema\")\n",
    "        \n",
    "        oracle_conn.close()  \n",
    "\n",
    "        # Renaming columns without special characters\n",
    "        df = remove_special_characters_from_all_column_names(df)\n",
    "\n",
    "        # Convert Column Data Types To Original\n",
    "        df = convert_data_types_to_original(original_schema_dict, df)\n",
    "\n",
    "        s3 = S3Hook(aws_conn_id='con-s3')\n",
    "        # Dump as parquet directly to S3:\n",
    "        with open(f\"s3://{target_bucket}/{file_key}\", 'wb', transport_params={'client': s3.get_conn()}) as out_file:\n",
    "            df.to_parquet(out_file, engine='pyarrow', index=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7cc8b",
   "metadata": {},
   "source": [
    "In this function, we converted oracle database to paraquet format and then transferred data to S3 storage. In order to do so, we first read the sql query and then used oracle hook to connect with oracle, and then fetched table meta data by calculating original schema which I already explained before. If the length of the original schema was zero, we raised an exception stating that we could not calculate original schema. Otherwise, we will rename columns without special characters, and then converted data types to original followed by dumping data as parquet directly to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Declare dummy start and stop tasks\n",
    "\n",
    "start_task = DummyOperator(task_id='start', dag=dag)\n",
    "\n",
    "start_dumping = DummyOperator(task_id='start_dumping', dag=dag)\n",
    "end_task = DummyOperator(task_id='end', dag=dag)\n",
    "\n",
    "checkpoint_count = 1\n",
    "\n",
    "\n",
    "def GetCheckPointTask(name):\n",
    "    return DummyOperator(task_id=name, dag=dag)\n",
    "\n",
    "\n",
    "# Step 4 - Read the list of elements from the airflow variable\n",
    "global_var = Variable.get(\"pos_multiple_dump_var\", deserialize_json=True)\n",
    "table_names = global_var['table_names']\n",
    "s3_dump_base_path = global_var['s3_dump_base_path']\n",
    "parallel_task_count = global_var['parallel_task_count']\n",
    "\n",
    "dump_tasks = []\n",
    "i = 0\n",
    "\n",
    "start_task >> start_dumping\n",
    "\n",
    "parent_task = start_dumping\n",
    "\n",
    "for val in table_names:\n",
    "    \n",
    "    val = val.lower()\n",
    "    res = val.split(\".\")\n",
    "    schema_name = res[0]\n",
    "    table = res[1]\n",
    "\n",
    "    task = PythonOperator(\n",
    "        task_id=f\"dump__{schema_name}.{table}\",\n",
    "        #trigger_rule=TriggerRule.ALL_DONE,\n",
    "        python_callable=OracleToParquetToS3, \n",
    "        op_kwargs={\n",
    "              \"schema_name\": schema_name,\n",
    "              \"table_name\": table,\n",
    "              \"target_bucket\": \"bigdata-dev-cmfcknil\", #Had to make a change here\n",
    "              \"file_key\":f\"{s3_dump_base_path}/{schema_name}/{table}.parquet\"\n",
    "        },\n",
    "        executor_config={\n",
    "            \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "            \"pod_override\": k8s.V1Pod(\n",
    "                spec=k8s.V1PodSpec(\n",
    "                    #node_selector={\n",
    "                     #   \"node-group\": \"master\"\n",
    "                    #},\n",
    "                    containers=[\n",
    "                        k8s.V1Container(\n",
    "                            name=\"base\",\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    i = i + 1\n",
    "    dump_tasks.append(task)\n",
    "\n",
    "\n",
    "\n",
    "# SEQUENTIAL TASKS\n",
    "if len(dump_tasks) > 0:\n",
    "    j = 0\n",
    "    for task in dump_tasks:\n",
    "        if j < parallel_task_count:\n",
    "            parent_task.set_downstream(dump_tasks[j])\n",
    "        else:\n",
    "            dump_tasks[j-parallel_task_count].set_downstream(dump_tasks[j]) \n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    x = parallel_task_count\n",
    "    while x > 0:\n",
    "        dump_tasks[j-parallel_task_count].set_downstream(end_task)\n",
    "        x = x - 1\n",
    "        j = j + 1\n",
    "    \n",
    "    #merge_tasks[j-1].set_downstream(end_task)\n",
    "else:\n",
    "    parent_task >> end_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be2227",
   "metadata": {},
   "source": [
    "This code clearly explains the mechanism of our work. First, we used dummy operators for initializing tasks and dumping databases. We also used dummy operators for ending the task. Then we defined check point task, and after that, we read the list of elements from variables where we set dependencies between starting_task and start_dumping and mentioned the dump path. Later, for every values in tables, we splitted full stop and then converted the database from oracle to parquet and then stored into S3 where I appended the task into list named dump_tasks. If the length of dump tasks was greater than zero, then we would iterate a dump task list and then check whether parallel task count would be greater than zero or not. If it were greater, then we would set parent-task into dump_tasks, otherwise we would set dump_tasks j - parallel task count to dump task. If the length of dump tasks was less than zero, then we would set dependencies between parent task and end task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76593cdf",
   "metadata": {},
   "source": [
    "## Explanation for the code of pos_multiple_dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec5f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is from new code (begin)\n",
    "\n",
    "\n",
    "dag = DAG(\n",
    "        dag_id='pos_large_INSTALLMENT_SCHE_dump',\n",
    "        start_date=datetime.datetime(2021, 12, 1),\n",
    "        default_args=default_args,\n",
    "        schedule_interval=\"@once\",\n",
    "        catchup=False,\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d87bb4",
   "metadata": {},
   "source": [
    "\n",
    "This code was used for declaring the dag id in order to dump multiple small tables of pos database into s3 storage. The dag_id aka dag name is pos_large_INSTALLMENT_SCHE_dump. The start_date marks the start of the DAG's first data interval, not when tasks in the dag will start running. The time is set with respect to the default timezone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0520ca76",
   "metadata": {},
   "source": [
    "The rest of the code was similar to the code written for pos multiple dump till step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ac344f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 3 - Declare dummy start and stop tasks\n",
    "\n",
    "start_task = DummyOperator(task_id='start', dag=dag)\n",
    "\n",
    "start_dumping = DummyOperator(task_id='start_dumping', dag=dag)\n",
    "\n",
    "end_task = DummyOperator(task_id='end', dag=dag)\n",
    "\n",
    "\n",
    "wait_to_dag_gets_updated = BashOperator(\n",
    "    task_id=\"wait_to_dag_gets_updated\",\n",
    "    dag=dag,\n",
    "    bash_command=\"sleep 3m\",\n",
    "    executor_config={\n",
    "        \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "        \"pod_override\": k8s.V1Pod(\n",
    "            spec=k8s.V1PodSpec(\n",
    "                #node_selector={\n",
    "                 #       \"node-group\": \"master\"\n",
    "                  #  },\n",
    "                containers=[\n",
    "                    k8s.V1Container(\n",
    "                        name=\"base\",\n",
    "                        image_pull_policy=\"Always\",\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87189100",
   "metadata": {},
   "source": [
    "First, we used dummyoperator to initialize the starting task and dump the tables. We also used dummyoperator to initialize the end task. Later on, we used bash operator in order to wait for the dag to get updated as we aimed to dump large tables in chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0081a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Read the list of elements from the airflow variable\n",
    "variable_name=\"pos_large_INSTALLMENT_SCHE_dump_var\"\n",
    "global_var = Variable.get(variable_name, deserialize_json=True)\n",
    "row_count = global_var['ROW_COUNT']['row_count'] #Made chanfe\n",
    "target_bucket_name = global_var['target_s3_bucket_name']\n",
    "s3_dump_base_path = global_var['s3_dump_base_path']\n",
    "dump_tasks = []\n",
    "total_fetched = 0\n",
    "chunk_size = 1000000    # 1 million\n",
    "\n",
    "val = global_var['table_name']\n",
    "#val = val.lower()\n",
    "res = val.split(\".\")\n",
    "schema_name = res[0]\n",
    "table = res[1]\n",
    "#schema_name=\"POS\" #According to this schema_name, a folder will get created\n",
    "#table=\"ACC_LEDGER\"\n",
    "\n",
    "def OracleToParquetToS3(schema_name, table_name, target_bucket, file_key, index, offset):\n",
    "    SQL= 'SELECT * FROM ' + schema_name+\".\"+table_name + f\" OFFSET {offset} ROWS FETCH NEXT {chunk_size} ROWS ONLY\"\n",
    "    oracle_conn = OracleHook(oracle_conn_id='con-ora-pos').get_conn()\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Executing SELECT Query...\")\n",
    "    df = pd.read_sql(SQL, oracle_conn)\n",
    "\n",
    "    ## Calculate Original Schema Dictionary\n",
    "    print(\"\")\n",
    "    print(\"Fetching Table Meta Data...\")\n",
    "    original_schema_dict = calculate_original_schema_dict(oracle_conn, schema_name, table_name)\n",
    "    if len(original_schema_dict.keys()) == 0:\n",
    "        raise Exception(\"Could not calculate original schema\")\n",
    "\n",
    "    oracle_conn.close()  \n",
    "\n",
    "    ## Renaming columns without special characters\n",
    "    df = remove_special_characters_from_all_column_names(df)\n",
    "\n",
    "    ## Convert Column Data Types To Original\n",
    "    df = convert_data_types_to_original(original_schema_dict, df)\n",
    "    \n",
    "    s3 = S3Hook(aws_conn_id='con-s3')\n",
    "    ## Dump as parquet directly to S3:\n",
    "    with open(f\"s3://{target_bucket}/{file_key}__{index}.parquet\", 'wb', transport_params={'client': s3.get_conn()}) as out_file:\n",
    "        df.to_parquet(out_file, engine='pyarrow', index=False)\n",
    "\n",
    "i = 1\n",
    "while total_fetched < row_count:\n",
    "    task = PythonOperator(\n",
    "        task_id=f\"dump__chunk_{i}\",\n",
    "        python_callable=OracleToParquetToS3, \n",
    "        op_kwargs={\n",
    "              \"schema_name\": schema_name,\n",
    "              \"table_name\": table,\n",
    "              \"target_bucket\": target_bucket_name,\n",
    "              \"file_key\":f\"{s3_dump_base_path}/{schema_name}/{table}\",\n",
    "              \"index\": i,\n",
    "              \"offset\": total_fetched\n",
    "        },\n",
    "        executor_config={\n",
    "            \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "            \"pod_override\": k8s.V1Pod(\n",
    "                spec=k8s.V1PodSpec(\n",
    "                   # node_selector={\n",
    "                    #    \"node-group\": \"master\"\n",
    "                   # },\n",
    "                    containers=[\n",
    "                        k8s.V1Container(\n",
    "                            name=\"base\",\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    dump_tasks.append(task)\n",
    "    i = i + 1\n",
    "    total_fetched = total_fetched + chunk_size\n",
    "\n",
    "\n",
    "# SEQUENTIAL TASKS\n",
    "if len(dump_tasks) > 0:\n",
    "    j = 0\n",
    "    for task in dump_tasks:\n",
    "        if j == 0:\n",
    "            start_dumping.set_downstream(dump_tasks[j])\n",
    "        else:\n",
    "            dump_tasks[j-1].set_downstream(dump_tasks[j]) \n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    dump_tasks[j-1].set_downstream(end_task)\n",
    "else:\n",
    "    start_dumping >> end_task\n",
    "\n",
    "\n",
    "\n",
    "def find_row_count_function():\n",
    "    count = 0\n",
    "    try:\n",
    "        count_query= 'SELECT COUNT(*) FROM ' + val #Made Change\n",
    "        oracle_hook = OracleHook(oracle_conn_id='con-ora-pos')\n",
    "        value = oracle_hook.get_first(sql = count_query)\n",
    "\n",
    "        count = value[0]\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        count = 0\n",
    "    \n",
    "    print(f\"Total Row Count: {count}\")\n",
    "    config = Variable.get(variable_name, deserialize_json=True) # Needto change variable name\n",
    "    config['ROW_COUNT']['row_count'] = count  #Made CHange\n",
    "    Variable.set(variable_name, json.dumps(config)) # Needto change variable name\n",
    "\n",
    "\n",
    "\n",
    "find_row_count = PythonOperator(\n",
    "    task_id=\"find_table_row_count\",\n",
    "    python_callable=find_row_count_function,\n",
    "    dag=dag,\n",
    "    executor_config={\n",
    "        \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "        \"pod_override\": k8s.V1Pod(\n",
    "            spec=k8s.V1PodSpec(\n",
    "                   #node_selector={\n",
    "                    #    \"node-group\": \"master\"\n",
    "                    #},\n",
    "                containers=[\n",
    "                    k8s.V1Container(\n",
    "                        name=\"base\",\n",
    "                        image_pull_policy=\"Always\",\n",
    "                    ),\n",
    "                ],\n",
    "            )\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "start_task >> find_row_count >> wait_to_dag_gets_updated >> start_dumping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef76a5",
   "metadata": {},
   "source": [
    "We read the list of elements from variables. Later, for every value in tables, we splitted full stop and then converted the database from oracle to parquet and then stored into S3 where I appended the task into list named dump_tasks and then measured the chunk size of thee tables which were dumped. If the length of dump tasks were greater than zero, then we would iterate a dump task list and then check whether it would be equal to zero or not. If it were zero, then we would set start_dumping into dump task, otherwise we would set downward dependencies between dump_tasks j - parallel task count to dump_task. After iterating a dump task list, we set downward dependencies between dump_task(j-1) to end_task, and then set dependencies between start_dumping and end_task. Then, we used row count function to count the number of rows for each table stored in S3 and then waited for dag to get updated. Finally, we set dependencies between start-task, find-row_count, wait_to_dag_gets_updated and start_dumping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4419347",
   "metadata": {},
   "source": [
    "## Explanation for the code of All_pos_s3_to_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "        dag_id='ALL_POS_S3_to_Iceberg_dag',\n",
    "        start_date=datetime.datetime(2021, 12, 1),\n",
    "        schedule_interval=\"@once\",\n",
    "        catchup=False,\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edb377",
   "metadata": {},
   "source": [
    "This code was used for declaring the dag id in order to merge multiple small tables in S3 storage to iceberg format. The dag_id aka dag name is ALL_POS_INSTALLMENT_SCHE_dump. The start_date marks the start of the DAG's first data interval, not when tasks in the dag will start running. The time is set with respect to the default timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db13b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Declare dummy start and stop tasks\n",
    "start_task = DummyOperator(task_id='start', dag=dag)\n",
    "start_merge_job = DummyOperator(task_id='start_merge_job', dag=dag)\n",
    "end_task = DummyOperator(task_id='end', dag=dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097a74b0",
   "metadata": {},
   "source": [
    "We used dummy operator for initializing a task. We also used dummy operator in order to commence merging a task and ending a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40352243",
   "metadata": {},
   "source": [
    "We have initialized check point equal to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9cd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GenerateRandomString():\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984cbdfb",
   "metadata": {},
   "source": [
    "This function randomly generates a string and converts the string into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4092572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCheckPointTask(name):\n",
    "    return DummyOperator(task_id=name, dag=dag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e22fc85",
   "metadata": {},
   "source": [
    "This function is used to get check points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285a6a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Read the list of elements from the airflow variable\n",
    "global_var = Variable.get(\"all_pos_s3_to_iceberg\", deserialize_json=True)\n",
    "inputs = global_var['inputs']\n",
    "parallel_task_count = global_var['parallel_task_count']\n",
    "min_executors = global_var['min_executors']\n",
    "max_executors = global_var['max_executors']\n",
    "executor_min_cpu = global_var['executor_min_cpu']\n",
    "executor_max_cpu = global_var['executor_max_cpu']\n",
    "executor_memory = global_var['executor_memory']\n",
    "s3_datasource_base_path = global_var['s3_datasource_base_path']\n",
    "\n",
    "merge_tasks = []\n",
    "i = 0\n",
    "\n",
    "start_task >> start_merge_job\n",
    "\n",
    "parent_task = start_merge_job\n",
    "\n",
    "for input in inputs:\n",
    "    #if i == 0:\n",
    "        #merge_tasks_group[dump_task_group_index] = []\n",
    "    \n",
    "    #val = val.lower()\n",
    "    #res = val.split(\".\")\n",
    "    #schema_name = res[0]\n",
    "    #table = res[1]\n",
    "\n",
    "    conf = {}\n",
    "    conf[\"min_executors\"] = min_executors\n",
    "    conf[\"max_executors\"] = max_executors\n",
    "    conf[\"executor_min_cpu\"] = executor_min_cpu\n",
    "    conf[\"executor_max_cpu\"] = executor_max_cpu\n",
    "    conf[\"executor_memory\"] = executor_memory\n",
    "    conf[\"s3_datasource_path\"] = f\"{s3_datasource_base_path}/{input['src']['db'].lower()}/{input['src']['schema'].lower()}/{input['src']['table'].lower()}.parquet\"\n",
    "    conf[\"target_database\"] = input['dst']['db'].lower()\n",
    "    conf[\"target_schema\"] = input['dst']['schema'].lower()\n",
    "    conf[\"target_table\"] = input['dst']['table'].lower()\n",
    "    conf[\"table_keys\"] = input['table_keys'].lower()\n",
    "    conf[\"update_date_column\"] = input['update_date_column'].lower() if 'update_date_column' in input else ''\n",
    "    conf[\"partition_column\"] = input['partition_column'].lower() if 'partition_column' in input else ''\n",
    "    conf[\"partition_column_transformation\"] = input['partition_column_transformation'].lower() if 'partition_column_transformation' in input else ''\n",
    "\n",
    "\n",
    "    task = TriggerDagRunOperator(\n",
    "        task_id=f\"merge__{input['src']['db'].lower()}.{input['src']['schema'].lower()}.{input['src']['table'].lower()}\",\n",
    "        trigger_dag_id=\"spark_submit_merge_parquet_to_iceberg\",\n",
    "        conf=conf,\n",
    "        dag=dag,\n",
    "        wait_for_completion=True,\n",
    "        executor_config={\n",
    "            \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "            \"pod_override\": k8s.V1Pod(\n",
    "                spec=k8s.V1PodSpec(\n",
    "                    containers=[\n",
    "                        k8s.V1Container(\n",
    "                            name=\"base\",\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    merge_tasks.append(task)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b34997",
   "metadata": {},
   "source": [
    "This code covers the full mechanism of merging small tables into iceberg format in S3 storage. In this step, we read the list of elements from airflow variable. First, we retrieved the variables from iceberg table where we declared minimum executors, maximum_executors, minimum cpu of executors, maximum cpu of executors, executor memory and s3 datasource base path. Subsequently, we declared merge task dictionary, and created dependencies between start-task and start_merge job. We then made start_merge job equal to parent task for performing sequntial task. Then for each table, we passed minimum executors, maximum executors, minimum cpu of executors, maximum cpu of executors, executor mememory and s3 datasource base path in configuration dictionary to perform configuration for the table. We also converted the destination path, table name and table schema into lowercase letters, and then we converted table keys and updated date column into lower case letter. Later, we partitioned the column and then converted the column name into lowercase letters. In order to merge the tables, we then used trigger dag run operator to convert the tables from parquet format to iceberg. After the conversion, we then appended the task with list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENTIAL TASKS\n",
    "if len(merge_tasks) > 0:\n",
    "    j = 0\n",
    "    for task in merge_tasks:\n",
    "        if j < parallel_task_count:\n",
    "            parent_task.set_downstream(merge_tasks[j])\n",
    "        else:\n",
    "            merge_tasks[j-parallel_task_count].set_downstream(merge_tasks[j]) \n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    x = parallel_task_count\n",
    "    while x > 0:\n",
    "        merge_tasks[j-parallel_task_count].set_downstream(end_task)\n",
    "        x = x - 1\n",
    "        j = j + 1\n",
    "    \n",
    "    #merge_tasks[j-1].set_downstream(end_task)\n",
    "else:\n",
    "    parent_task >> end_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4415d73",
   "metadata": {},
   "source": [
    "Then we checked whether the length of merge task would be greater than zero. If it were greater than zero, we would iterate a merge task list and then check whether parallel task count would be greater than j or not. j is like a count function. If parallel task count were greater than j, then we would set downnward dependencies between parent_task and merge_tasks, otherwise,  we would set downward dependencies between merge_tasks[j-parallel_task_count] with merge tasks. After completing iterating merge tasks list, we initialized parallel_task_count with x and then decremented x till 1. Inside the loop, we set downward dependencies between merge_tasks[j-parallel_task_count] with end task. If the length of merge tasks were less than zero, we would set dependencies between parent_task and end_task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c0383a",
   "metadata": {},
   "source": [
    "## Explanation for the code of pos_large_pos_installment_sche_iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fde981",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag = DAG(\n",
    "        dag_id='pos_large_installment_sche_ICEBERG', #NEED TO CHANGE HERE\n",
    "        start_date=datetime.datetime(2021, 12, 1),\n",
    "        schedule_interval=\"@once\",\n",
    "        default_args = default_args,\n",
    "        catchup=False,\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4057a5",
   "metadata": {},
   "source": [
    "This code was used for declaring the dag id in order to merge a large table in S3 storage to iceberg format. The dag_id aka dag name is pos_large_pos_installment_sche_iceberg. The start_date marks the start of the DAG's first data interval, not when tasks in the dag will start running. The time is set with respect to the default timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086309a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Declare dummy start and stop tasks\n",
    "start_task = DummyOperator(task_id='start', dag=dag)\n",
    "start_insert_job = DummyOperator(task_id='start_insert_job', dag=dag)\n",
    "end_task = DummyOperator(task_id='end', dag=dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f87eb",
   "metadata": {},
   "source": [
    "We used dummy operator for initializing a task. We also used dummy operator in order to commence insertion and ending a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7626b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Read the list of elements from the airflow variable\n",
    "\n",
    "parallel_task_count = 1\n",
    "min_executors = \"2\"\n",
    "max_executors = \"6\"\n",
    "executor_min_cpu = \"1\"\n",
    "executor_max_cpu = \"2\"\n",
    "executor_memory = \"12G\"\n",
    "s3_datasource_base_path = \"bigdata-dev-cmfcknil/raw/idl/posdb/POS_INSTALLMENT_SCHE/POS\"# AND HERE (Object storage location of the file)\n",
    "part_start_index = 1\n",
    "part_end_index = 37 # NEED TO CHANGE THIS ACCORDING TO NUMBER OF CHUNKS\n",
    "\n",
    "tasks = []\n",
    "\n",
    "i = 0\n",
    "j = part_start_index\n",
    "\n",
    "start_task >> start_insert_job\n",
    "\n",
    "parent_task = start_insert_job\n",
    "\n",
    "while j <= part_end_index:\n",
    "    \n",
    "    #val = val.lower()\n",
    "    #res = val.split(\".\")\n",
    "    #schema_name = res[0]\n",
    "    #table = res[1]\n",
    "\n",
    "    conf = {}\n",
    "    conf[\"min_executors\"] = min_executors\n",
    "    conf[\"max_executors\"] = max_executors\n",
    "    conf[\"executor_min_cpu\"] = executor_min_cpu\n",
    "    conf[\"executor_max_cpu\"] = executor_max_cpu\n",
    "    conf[\"executor_memory\"] = executor_memory\n",
    "    conf[\"s3_datasource_path\"] = f\"{s3_datasource_base_path}/POS_INSTALLMENT_SCHE__{j}.parquet\" #ALSO HERE, Take the table name from S3 object storage\n",
    "    conf[\"target_database\"] = \"pos\" # Creates this folder under warehouse if not exists\n",
    "    conf[\"target_schema\"] = \"pos\" # Appends this schema name with the tablename\n",
    "    conf[\"target_table\"] = \"pos_installment_sche\" #AND HERE, DESTINATION (keep the name in small letter)\n",
    "    conf[\"table_keys\"] = \"ID\"\n",
    "    conf[\"update_date_column\"] = \"MODIFIED\" #Taken from control tableb\n",
    "    conf[\"partition_column\"] = \"INSTALLMENT_DUE_DATE\" #Taken from control table\n",
    "    conf[\"partition_column_transformation\"] = \"\"\n",
    "\n",
    "\n",
    "    task = TriggerDagRunOperator(\n",
    "        task_id=f\"merge__part_{j}\",\n",
    "        trigger_dag_id=\"spark_submit_merge_parquet_to_iceberg\",\n",
    "        conf=conf,\n",
    "        dag=dag,\n",
    "        wait_for_completion=True,\n",
    "        executor_config={\n",
    "            \"pod_template_file\": os.path.join(AIRFLOW_HOME, \"kubernetes/pod_templates/default_template_2.yaml\"),\n",
    "            \"pod_override\": k8s.V1Pod(\n",
    "                spec=k8s.V1PodSpec(\n",
    "                    containers=[\n",
    "                        k8s.V1Container(\n",
    "                            name=\"base\",\n",
    "                            resources = k8s.V1ResourceRequirements(\n",
    "                                requests= {\n",
    "                                    \"cpu\": \"200m\",\n",
    "                                    \"memory\": \"1G\"\n",
    "                                },\n",
    "                                limits = {\n",
    "                                    \"cpu\": \"1\",\n",
    "                                    \"memory\": \"1G\"\n",
    "                                }\n",
    "                            ),\n",
    "                        ),\n",
    "                    ],\n",
    "                )\n",
    "            ),\n",
    "        },\n",
    "    )\n",
    "\n",
    "    i = i + 1\n",
    "    j = j + 1\n",
    "    tasks.append(task)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c00d6b",
   "metadata": {},
   "source": [
    "First, we declared default parallel task count. Then, we declared minimum_executors,maximum_executors,executor_minimum_cpu,executor_maximum_cpu,executor_memory,S3_database_cource_path,part_start_index,part_end_index for merging parquet file into iceberg format. We then iterated each table from starting index to end index to set the declared values in order to configure our tables, and then, we used trigger dag operaator to convert parquet tables into iceberg format. We then appended the task with list array named tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd8cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENTIAL TASKS\n",
    "if len(tasks) > 0:\n",
    "    j = 0\n",
    "    for task in tasks:\n",
    "        if j < parallel_task_count:\n",
    "            parent_task.set_downstream(tasks[j])\n",
    "        else:\n",
    "            tasks[j-parallel_task_count].set_downstream(tasks[j]) \n",
    "\n",
    "        j = j + 1\n",
    "\n",
    "    x = parallel_task_count\n",
    "    while x > 0:\n",
    "        tasks[j-parallel_task_count].set_downstream(end_task)\n",
    "        x = x - 1\n",
    "        j = j + 1\n",
    "    \n",
    "    #convert_tasks[j-1].set_downstream(end_task)\n",
    "else:\n",
    "    parent_task >> end_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8689f6",
   "metadata": {},
   "source": [
    "Then we would check whether the length of the tasks would be greater than zero or not. If it were greater than zero, then we would iterate the list and check whether parallel task count would be greater than zero. If it were greater than zero, then we would set downward dependencies between parent task and tasks, otherwise, we would set downward dependencies between tasks j- parallel_task_count and tasks. Then, we initialized x to parallel_task_count. If x were greater than 0, then we would set downward dependencies between tasks[j-parallel_task_count] with end_task. If the length of the tasks were less than zero, then we would set dependencies between parent_task and end_task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ed9e17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
